{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki = load_dataset(\"jerteh/SrpWiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3818535\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UTF-8 UTF-8 varijanta je najzgodnija za kodiranje većinski latiničnog teksta.', 'Dato je i kratko uputstvo za korišćenje te varijante u Microsoft Word-u, Netscape Composer-u i tekstualnom editoru Kate.', 'U tekstu su takođe preporučeni standardni Unicode fontovi koji omogućavaju laku prenosivost teksta sa računara na računar ili za objavljivanje teksta na Internet.', 'Prvi računari su bili pravljeni pretežno za englesko govorno područje i imali su podršku samo za engleski alfabet, za brojeve, zagrade i još po neki kontrolni karakter, što je činilo ukupno 128 mogućih slova (u 7 bita).', 'To je bio tzv. \"-{ASCII}-\" ili \"-{US-ASCII}-\" standard.', '1968. godine je skup karaktera proširen na 256 (8 bita), a \"gornjih\" 128 karaktera je bilo korišćeno za dodatne karaktere.', 'Iz neke navike je i ovaj prošireni \"-{ASCII}-\" nazivan \"-{ASCII}-\", tako da tu često dolazi do zabune.', 'Da bi postojala podrška za više jezika, smišljane su tzv. kodne strane (\"-{Code Page}-\") koje definišu ponašanje tog dodatnog skupa slova.', 'Osnovna kodna strana na personalnim računarima (\"-{PC437}-\") u tom gornjem setu karaktera definiše razne grafičke karaktere za crtanje tekstualnih prozora i slično.', 'Kasnije je razvijeno još puno kodnih strana koje podržavaju određene jezike.']\n",
      "3818535\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "text_wiki = dataset_wiki['train']['text']\n",
    "print(text_wiki[:10])\n",
    "print(len(text_wiki))\n",
    "print(type(text_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_news = load_dataset(\"jerteh/SrpKorNews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 337\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "text_news = dataset_news['train']['text']\n",
    "print(len(text_news))\n",
    "print(type(text_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sci = load_dataset(\"procesaur/STARS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 7559526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sci = dataset_sci['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Uticaj brzine vazduha na aerosole formirane od 1 % emulzija 112 Uticaj brzine vazduha na aerosole formirane od 6 % emulzija 119 Uticaj brzine vazduha na aerosole formirane od 10 % emulzija 128 Strujanje aerosola vode u cevi</s>\n",
      "7559526\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(text_sci[0])\n",
    "print(len(text_sci))\n",
    "print(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_mini = text_news[:1]\n",
    "sci_mini = text_sci[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentnecs(texts):\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        matches = re.findall(r'<s>(.*?)</s>', text)\n",
    "        sentences.extend(matches)\n",
    "    \n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate one function with create_sentences and flag for loop if dataset isWiki? \n",
    "def return_dic(sentences):\n",
    "    sent_lst_dic = [{'sentence': x, 'id' : i} for i, x in enumerate(sentences)]\n",
    "    return sent_lst_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0,\n",
      "  'sentence': 'UTF-8 UTF-8 varijanta je najzgodnija za kodiranje većinski '\n",
      "              'latiničnog teksta.'},\n",
      " {'id': 1,\n",
      "  'sentence': 'Dato je i kratko uputstvo za korišćenje te varijante u '\n",
      "              'Microsoft Word-u, Netscape Composer-u i tekstualnom editoru '\n",
      "              'Kate.'},\n",
      " {'id': 2,\n",
      "  'sentence': 'U tekstu su takođe preporučeni standardni Unicode fontovi koji '\n",
      "              'omogućavaju laku prenosivost teksta sa računara na računar ili '\n",
      "              'za objavljivanje teksta na Internet.'}]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "sent_wiki = return_dic(text_wiki[:30])\n",
    "pprint(sent_wiki[:3])\n",
    "print(len(sent_wiki))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'sentence': 'Mečka na usijanom limenom krovu'},\n",
      " {'id': 1,\n",
      "  'sentence': 'Piše: Dušan Vidaković Kada je austrijski konzul u Nici Emil '\n",
      "              'Jelinek 1901. naručio od nemačkog Dajmlera, odjednom, čak 36 '\n",
      "              'automobila, čija je ukupna cena premašila fantastičnih 550 '\n",
      "              'hiljada zlatnih maraka, imao je samo jedan uslov: da sva ta '\n",
      "              'kola nose ime njegove ćerke.'},\n",
      " {'id': 2,\n",
      "  'sentence': 'Ljudi iz kanstatske firme, naravno, nisu imali ništa protiv, a '\n",
      "              'istorija automobilizma je imala sreću da je Jelinekovoj '\n",
      "              'mezimici kum dao (pogotovo za Špance) vrlo graciozno ime - '\n",
      "              'Mercedes, što na jeziku konkvistadora znaci ljupkost, umilnost, '\n",
      "              'prijatnost.'}]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "sent_news = create_sentnecs(news_mini)\n",
    "sent_news = return_dic(sent_news[:100])\n",
    "pprint(sent_news[:3])\n",
    "print(len(sent_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0,\n",
      "  'sentence': 'Uticaj brzine vazduha na aerosole formirane od 1 % emulzija 112 '\n",
      "              'Uticaj brzine vazduha na aerosole formirane od 6 % emulzija 119 '\n",
      "              'Uticaj brzine vazduha na aerosole formirane od 10 % emulzija '\n",
      "              '128 Strujanje aerosola vode u cevi'},\n",
      " {'id': 1,\n",
      "  'sentence': 'Završna razmatranja strujanja aerosola u cevi formiranog '\n",
      "              'emulzije EO3'},\n",
      " {'id': 2,\n",
      "  'sentence': 'Moguće strukture aerosol čestice NaCl: a) neporozna čvrsta, b) '\n",
      "              'porozna čvrsta, c) čvrsta sa otvorenim džepovima rastovrenog '\n",
      "              'NaCl, d) čvrsta čaura sa vodenim jezgrom, e) čvrsta NaCl '\n",
      "              'čestica sa jezgrom vodenog rastvora NaCl, f) vodena kapljica '\n",
      "              'NaCl.'}]\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "sent_sci = create_sentnecs(sci_mini)\n",
    "sent_sci = return_dic(sent_sci)\n",
    "pprint(sent_sci[:3])\n",
    "print(len(sent_sci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating combined sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m oaiembeds \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "oaiembeds = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can this be improved?\n",
    "def combine_sentences(sentences, buffer_size=buffer_size):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(sentences):\n",
    "    embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why convert cosine similarity to cosine distance?\n",
    "# just use cosine similarity instead?\n",
    "# test on couple of examples and see if it makes a difference\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoint(sentences, distances, treshold):\n",
    "    # We need to get the distance threshold that we'll consider an outlier\n",
    "    # We'll use numpy .percentile() for this\n",
    "    breakpoint_percentile_threshold = treshold # the percentile is determined based on numpy viasualization I guess? no? how do you determine percentile?\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "\n",
    "    # Then we'll see how many distances are actually above this one\n",
    "    num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "\n",
    "    # Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "    # Initialize the start index\n",
    "    start_index = 0\n",
    "\n",
    "    # Create a list to hold the grouped sentences\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate through the breakpoints to slice the sentences\n",
    "    for index in indices_above_thresh:\n",
    "        # The end index is the current breakpoint\n",
    "        end_index = index\n",
    "\n",
    "        # Slice the sentence_dicts from the current start index to the end index\n",
    "        group = sentences[start_index:end_index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        \n",
    "        # Update the start index for the next group\n",
    "        start_index = index + 1\n",
    "\n",
    "    # The last group, if any sentences remain\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, sentences = calculate_cosine_distances(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selena\n",
    "SYSTEN_PROMPT_QUERIES = \"\"\"\n",
    "### Goal ###\n",
    "Tre primary objective is to produce multiple queries in the Serbian language and a list of keywords in the Serbian language from the provided context. The context repesents an answer to the query and the keywords best \n",
    "describe the context. The goal is to have query-context pairs that corelate with each other and a list of keywords that would spead-up the search in the future.\n",
    "\n",
    "### Process Overview ###\n",
    "1. Carefully read and analyze the given context text.\n",
    "2. Identify all relevant keywords and what the context text is about.\n",
    "3. Find the queries that best represents the given context text.\n",
    "\n",
    "### Formatting Rules ###\n",
    "- Keyword value MUST be a LIST of strings with max lenght of 5 or [null] if no relevant information is provided.\n",
    "- Use double quotes for strings and escape internal quotes with a backslash (\\).\n",
    "- Keep the queries concise and general about the context text.\n",
    "- Ensure the output is a valid JSON file, parsable by Python's json.loads().\n",
    "- Strictly use only the information provided in the context text. Do not add, infer, or imagine any additional details beyond what is explicitly stated.\n",
    "- Remember to answer in Serbian.\n",
    "\n",
    "### Query description###\n",
    "- All queries must be WH-questions.\n",
    "- All queries must begin with a capital letter and end with a questoin mark (?).\n",
    "\n",
    "### Output Format ###\n",
    "{{\n",
    " \"keywords\": [\"The keyword that best represent the given context with max lenght of 5\"],\n",
    " \"short_query\": \"A short query that best suits the given context. It should be only a few words long and general.\",\n",
    " \"medium_query\": \"A minium lenght query that best suits the given context. It should be a lenght of min 10 words and max 18.\",\n",
    " \"long_query\": \"A long query that best suits the given context. It should be longer than 19 words and very specific to the context.\"\n",
    "}}\n",
    "\n",
    "### Context ###\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selena\n",
    "# def test_api(prompt, context, model: str = \"gpt-3.5-turbo-0125\"):\n",
    "#     client = openai\n",
    "\n",
    "\n",
    "#     # Assuming OpenAI API key is set elsewhere\n",
    "#     # Initiate the streaming response\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": prompt},\n",
    "#             {\"role\": \"user\", \"content\": context}\n",
    "#         ],\n",
    "#         response_format={'type': 'json_object'},\n",
    "#         temperature=0,\n",
    "#         top_p=1\n",
    "#     )\n",
    "\n",
    "#     return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selena\n",
    "\n",
    "# def make_dataset(file_path: Path):\n",
    "#         returned_dict = {\n",
    "#              \"context\": [],\n",
    "#              \"short_query\": [],\n",
    "#              \"medium_query\": [],\n",
    "#              \"long_query\": [],\n",
    "#              \"keywords\": []\n",
    "#         }\n",
    "#         # Open and iterate through the .jsonl file\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             for line in file:\n",
    "#                 data = json.loads(line)\n",
    "#                 context = data[-1]['context']\n",
    "#                 returned_data = data[1]['choices'][0]['message']['content']\n",
    "#                 returned_data = json.loads(returned_data)\n",
    "#                 returned_dict['context'].append(context)\n",
    "#                 returned_dict['short_query'].append(returned_data['short_query'])\n",
    "#                 returned_dict['medium_query'].append(returned_data['medium_query'])\n",
    "#                 returned_dict['long_query'].append(returned_data['long_query'])\n",
    "#                 returned_dict['keywords'].append(returned_data['keywords'])\n",
    "#         return returned_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
