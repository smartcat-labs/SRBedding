{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'data'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "# Specify the path to the JSON file\n",
    "file_path = 'datasets/squad-sr-lat.json'\n",
    "\n",
    "# Open the JSON file and load its contents\n",
    "with open(file_path, 'r', encoding='utf8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now, 'data' contains the parsed JSON data as a Python dictionary\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = []\n",
    "sentence2 = []\n",
    "data_for_eval = {\n",
    "    'id': [],\n",
    "    'title': [],\n",
    "    'context': [],\n",
    "    'question': [],\n",
    "    'answers': []\n",
    "}\n",
    "i = 1\n",
    "for article in data['data']:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa_pair in paragraph['qas']:\n",
    "            question = qa_pair['question']\n",
    "            data_for_answer = {\n",
    "                \"text\": [],\n",
    "                'answer_start': []\n",
    "            }\n",
    "            for answer in qa_pair['answers']:\n",
    "                short_answer = answer['text']\n",
    "                answer_start = answer['answer_start']\n",
    "                sentence1.append(question)\n",
    "                sentence2.append(short_answer)\n",
    "                data_for_answer['text'].append(short_answer)\n",
    "                data_for_answer['answer_start'].append(answer_start)\n",
    "            data_for_eval['id'].append(str(i))\n",
    "            data_for_eval['title'].append(title)\n",
    "            data_for_eval['context'].append(context)\n",
    "            data_for_eval['question'].append(question)\n",
    "            data_for_eval['answers'].append(data_for_answer)\n",
    "            i += 1\n",
    "                # print(answer)\n",
    "dataset_for_eval = Dataset.from_dict(data_for_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/selena/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb5b699d6b84f92aac94c0b4e2bce6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36597ecefb714673b6484518ad7b568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator(\"question-answering\")\n",
    "# data = load_dataset(\"squad_v2\", split=\"validation[:2]\")\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"classla/bcms-bertic\",\n",
    "    data=dataset_for_eval,\n",
    "    metric=\"squad_v2\",\n",
    "    squad_v2_format=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 0.0,\n",
       " 'f1': 4.2324445484296795,\n",
       " 'total': 269,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 4.2324445484296795,\n",
       " 'HasAns_total': 269,\n",
       " 'best_exact': 0.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 4.232444548429679,\n",
       " 'best_f1_thresh': 0.00017354990995954722,\n",
       " 'total_time_in_seconds': 56.48018586899707,\n",
       " 'samples_per_second': 4.762732201765976,\n",
       " 'latency_in_seconds': 0.20996351624162482}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the data to wanted format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "titles = []\n",
    "contexts = []\n",
    "queryes = []\n",
    "for article in data['data']:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        currect_queryes = []\n",
    "        for qa_pair in paragraph['qas']:\n",
    "            question = qa_pair['question']\n",
    "            \n",
    "            currect_queryes.append(question)\n",
    "            i += 1\n",
    "        titles.append(title)\n",
    "        contexts.append(context)\n",
    "        queryes.append(currect_queryes)\n",
    "                # print(answer)\n",
    "dataset_for_eval = Dataset.from_dict(data_for_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kome se u 1858. godini u Lurdu, Francuska, navodno pojavila Deva Marija?',\n",
       " 'Šta je ispred Glavne zgrade Notr Dame?',\n",
       " 'Bazilika Svetog srca u Notr Dami je pored koje strukture?',\n",
       " 'Šta je Grota u Notr Dami?',\n",
       " 'Šta se nalazi na vrhu Glavne zgrade u Notr Damu?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from collections import defaultdict\n",
    "  \n",
    "dataset_for_eval_df = pd.DataFrame({'title' : titles,\n",
    "                                    'cotext' : contexts,\n",
    "                                  #  'query_id' : dataset_for_eval['id'],\n",
    "                                    'query' : queryes})\n",
    "\n",
    "dataset_for_eval_df.head()\n",
    "dataset_for_eval_df['query'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_eval_df.to_csv('datasets/dataset_procesed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18896 entries, 0 to 18895\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   18896 non-null  object\n",
      " 1   cotext  18896 non-null  object\n",
      " 2   query   18896 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 443.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_for_eval_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the pipieline for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selena/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': np.float64(0.0), 'f1_score': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "titles = [\"University_of_Notre_Dame\", \"Normans\"]\n",
    "contexts = [\n",
    "    \"Arhitektonski, škola ima katolički karakter...\",\n",
    "    \"The Normans (Norman: Nourmands; French: Normands...)\"\n",
    "]\n",
    "questions = [\n",
    "    \"Kome se u 1858. godini u Lurdu, Francuska, navodno pojavila Deva Marija?\",\n",
    "    \"In what country is Normandy located?\"\n",
    "]\n",
    "answers = [\n",
    "    {\"text\": [\"Svetoj Bernadet Subiru\"], \"answer_start\": [464]},\n",
    "    {\"text\": [\"France\"], \"answer_start\": [159]}\n",
    "]\n",
    "\n",
    "# Prepare the dataset\n",
    "data_for_eval = {\n",
    "    'id': [str(i) for i in range(len(titles))],\n",
    "    'title': titles,\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answers': answers\n",
    "}\n",
    "\n",
    "dataset_for_eval = Dataset.from_dict(data_for_eval)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"classla/bcms-bertic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate predictions\n",
    "def generate_predictions(dataset):\n",
    "    predictions = []\n",
    "    for example in dataset:\n",
    "        prediction = qa_pipeline(question=example['question'], context=example['context'])\n",
    "        predictions.append({\n",
    "            'id': example['id'],\n",
    "            'prediction_text': prediction['answer'],\n",
    "            'no_answer_probability': prediction.get('score', 0.0)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(dataset_for_eval)\n",
    "\n",
    "# Custom evaluation metrics\n",
    "def compute_custom_metrics(predictions, references):\n",
    "    # Example metric: Exact Match (EM) and F1 Score\n",
    "    # def exact_match_score(prediction, ground_truth):\n",
    "    #     return int(prediction == ground_truth)\n",
    "\n",
    "    def f1_score(prediction, ground_truth):\n",
    "        prediction_tokens = prediction.split()\n",
    "        ground_truth_tokens = ground_truth.split()\n",
    "        common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "        if len(common) == 0:\n",
    "            return 0\n",
    "        precision = len(common) / len(prediction_tokens)\n",
    "        recall = len(common) / len(ground_truth_tokens)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    em = []\n",
    "    f1 = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        ground_truth_text = ref['text'][0]  # assuming there's only one ground truth answer\n",
    "        em.append(exact_match_score(pred['prediction_text'], ground_truth_text))\n",
    "        f1.append(f1_score(pred['prediction_text'], ground_truth_text))\n",
    "    \n",
    "    return {\n",
    "        'exact_match': np.mean(em),\n",
    "        'f1_score': np.mean(f1)\n",
    "    }\n",
    "\n",
    "# References from the dataset\n",
    "references = [{'id': example['id'], 'text': example['answers']['text']} for example in dataset_for_eval]\n",
    "\n",
    "# Compute custom metrics\n",
    "results = compute_custom_metrics(predictions, references)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selena/sele/SRBedding/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': np.float64(0.0), 'f1_score': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import pytrec_eval\n",
    "\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "titles = [\"University_of_Notre_Dame\", \"Normans\"]\n",
    "contexts = [\n",
    "    \"Arhitektonski, škola ima katolički karakter...\",\n",
    "    \"The Normans (Norman: Nourmands; French: Normands...)\"\n",
    "]\n",
    "questions = [\n",
    "    \"Kome se u 1858. godini u Lurdu, Francuska, navodno pojavila Deva Marija?\",\n",
    "    \"In what country is Normandy located?\"\n",
    "]\n",
    "\n",
    "# Assume these are the ground truth answers for evaluation purposes\n",
    "reference_answers = [\n",
    "    \"Svetoj Bernadet Subiru\",\n",
    "    \"France\"\n",
    "]\n",
    "\n",
    "# Prepare the dataset\n",
    "data_for_eval = {\n",
    "    'id': [str(i) for i in range(len(titles))],\n",
    "    'title': titles,\n",
    "    'context': contexts,\n",
    "    'question': questions\n",
    "}\n",
    "\n",
    "dataset_for_eval = Dataset.from_dict(data_for_eval)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"classla/bcms-bertic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "# Create model embeddings\n",
    "# def generate_embeddings(text):\n",
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "# Measuring query-context similarity\n",
    "# def compute_similarity(query, context, top_k):\n",
    "# Generate predictions\n",
    "def generate_predictions(dataset):\n",
    "    predictions = []\n",
    "    for example in dataset:\n",
    "        prediction = qa_pipeline(question=example['question'], context=example['context'])\n",
    "        predictions.append({\n",
    "            'id': example['id'],\n",
    "            'prediction_text': prediction['answer']\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(dataset_for_eval)\n",
    "\n",
    "# Custom evaluation metrics\n",
    "def compute_custom_metrics(predictions, references):\n",
    "    # Example metric: Exact Match (EM) and F1 Score\n",
    "    def precision_k(prediction, ground_truth, k):\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(ground_truth, {'P.4'})\n",
    "        results = evaluator.evaluate(prediction)\n",
    "        return results\n",
    "\n",
    "    def f1_score(prediction, ground_truth):\n",
    "        prediction_tokens = prediction.split()\n",
    "        ground_truth_tokens = ground_truth.split()\n",
    "        common = set(prediction_tokens) & set(ground_truth_tokens)\n",
    "        if len(common) == 0:\n",
    "            return 0\n",
    "        precision = len(common) / len(prediction_tokens)\n",
    "        recall = len(common) / len(ground_truth_tokens)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    em = []\n",
    "    f1 = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        em.append(exact_match_score(pred['prediction_text'], ref))\n",
    "        f1.append(f1_score(pred['prediction_text'], ref))\n",
    "    \n",
    "    return {\n",
    "        'exact_match': np.mean(em),\n",
    "        'f1_score': np.mean(f1)\n",
    "    }\n",
    "\n",
    "# Compute custom metrics\n",
    "results = compute_custom_metrics(predictions, reference_answers)\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
