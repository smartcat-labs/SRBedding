{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'data'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "# Specify the path to the JSON file\n",
    "file_path = 'datasets/squad-sr-lat.json'\n",
    "\n",
    "# Open the JSON file and load its contents\n",
    "with open(file_path, 'r', encoding='utf8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now, 'data' contains the parsed JSON data as a Python dictionary\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 = []\n",
    "# sentence2 = []\n",
    "# data_for_eval = {\n",
    "#     'id': [],\n",
    "#     'title': [],\n",
    "#     'context': [],\n",
    "#     'question': [],\n",
    "#     'answers': []\n",
    "# }\n",
    "# i = 1\n",
    "# for article in data['data']:\n",
    "#     title = article['title']\n",
    "#     for paragraph in article['paragraphs']:\n",
    "#         context = paragraph['context']\n",
    "#         for qa_pair in paragraph['qas']:\n",
    "#             question = qa_pair['question']\n",
    "#             data_for_answer = {\n",
    "#                 \"text\": [],\n",
    "#                 'answer_start': []\n",
    "#             }\n",
    "#             for answer in qa_pair['answers']:\n",
    "#                 short_answer = answer['text']\n",
    "#                 answer_start = answer['answer_start']\n",
    "#                 sentence1.append(question)\n",
    "#                 sentence2.append(short_answer)\n",
    "#                 data_for_answer['text'].append(short_answer)\n",
    "#                 data_for_answer['answer_start'].append(answer_start)\n",
    "#             data_for_eval['id'].append(str(i))\n",
    "#             data_for_eval['title'].append(title)\n",
    "#             data_for_eval['context'].append(context)\n",
    "#             data_for_eval['question'].append(question)\n",
    "#             data_for_eval['answers'].append(data_for_answer)\n",
    "#             i += 1\n",
    "#                 # print(answer)\n",
    "# dataset_for_eval = Dataset.from_dict(data_for_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import evaluator\n",
    "# task_evaluator = evaluator(\"question-answering\")\n",
    "# # data = load_dataset(\"squad_v2\", split=\"validation[:2]\")\n",
    "# results = task_evaluator.compute(\n",
    "#     model_or_pipeline=\"classla/bcms-bertic\",\n",
    "#     data=dataset_for_eval,\n",
    "#     metric=\"squad_v2\",\n",
    "#     squad_v2_format=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the data to wanted format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "titles = []\n",
    "contexts = []\n",
    "queryes = []\n",
    "for article in data['data']:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        currect_queryes = []\n",
    "        for qa_pair in paragraph['qas']:\n",
    "            question = qa_pair['question']\n",
    "            \n",
    "            currect_queryes.append(question)\n",
    "            i += 1\n",
    "        titles.append(title)\n",
    "        contexts.append(context)\n",
    "        queryes.append(currect_queryes)\n",
    "                # print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kome se u 1858. godini u Lurdu, Francuska, navodno pojavila Deva Marija?',\n",
       " 'Šta je ispred Glavne zgrade Notr Dame?',\n",
       " 'Bazilika Svetog srca u Notr Dami je pored koje strukture?',\n",
       " 'Šta je Grota u Notr Dami?',\n",
       " 'Šta se nalazi na vrhu Glavne zgrade u Notr Damu?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from collections import defaultdict\n",
    "  \n",
    "dataset_for_eval_df = pd.DataFrame({'title' : titles,\n",
    "                                    'context' : contexts,\n",
    "                                  #  'query_id' : dataset_for_eval['id'],\n",
    "                                    'queries' : queryes})\n",
    "\n",
    "dataset_for_eval_df.head()\n",
    "dataset_for_eval_df['queries'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "dataset_for_eval_df.to_csv('datasets/dataset_processed.csv', index=False)\n",
    "dataset_for_eval_df.to_pickle('dataset_for_eval.pkl')\n",
    "table = pa.Table.from_pandas(dataset_for_eval_df)\n",
    "\n",
    "pq.write_table(table, 'datasets/dataset_processed.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18896 entries, 0 to 18895\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    18896 non-null  object\n",
      " 1   context  18896 non-null  object\n",
      " 2   queries  18896 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 443.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_for_eval_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the pipieline for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model embeddings\n",
    "# def generate_embeddings(text):\n",
    "# Initialize the question-answering pipeline\n",
    "# Measuring query-context similarity\n",
    "# def compute_similarity(query, context, top_k):\n",
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model_name = \"classla/bcms-bertic\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('datasets/dataset_processed.csv')\n",
    "\n",
    "# # Function to get the answer from the context based on the predicted start and end positions\n",
    "# def get_answer_from_positions(context, start_pos, end_pos):\n",
    "#     return context[start_pos:end_pos+1]\n",
    "\n",
    "# # Tokenize the context and query, and get predictions\n",
    "# def get_predictions(context, query):\n",
    "#     inputs = tokenizer.encode_plus(query, context, return_tensors='pt')\n",
    "#     input_ids = inputs['input_ids']\n",
    "#     token_type_ids = inputs['token_type_ids']\n",
    "#     attention_mask = inputs['attention_mask']\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "#         start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "    \n",
    "#     start_pos = torch.argmax(start_scores)\n",
    "#     end_pos = torch.argmax(end_scores)\n",
    "    \n",
    "#     answer_tokens = input_ids[0][start_pos:end_pos+1]\n",
    "#     answer = tokenizer.decode(answer_tokens)\n",
    "#     return answer\n",
    "\n",
    "# # Evaluate the model\n",
    "# predictions = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     context = row['context']\n",
    "#     queries = row['queries']\n",
    "#     for query in queries:\n",
    "#         prediction = get_predictions(context, query)\n",
    "#         predictions.append(prediction)\n",
    "\n",
    "# df['predicted_answer'] = predictions\n",
    "\n",
    "# # (Optional) Save the predictions to a new CSV file\n",
    "# df.to_csv('dataset_with_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\n",
    "from collections import defaultdict\n",
    "import pytrec_eval\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ucitati model i tokenizer\n",
    "2. ucitati df\n",
    "3. prolazimo kroz sve recorde u df i za svaki record\n",
    "    1. ubacimo listu query u uciran model i dobije listu embedinga\n",
    "    2. ubacimo context u model i dobijemo encodirano\n",
    "    3. merimo slicnost izmedju embedinga i matrice vecContexta@matriceVektoraQuerya\n",
    "    pored cosinusne se cuva i index contexta\n",
    "    nama treba i za query da se cuva koji je njegog paragraf\n",
    "    4. sortiramo sve umnoske, iyvucemo najslicniji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Arhitektonski, škola ima katolički karakter. N...</td>\n",
       "      <td>[Kome se u 1858. godini u Lurdu, Francuska, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Kao i na većini drugih univerziteta, studenti ...</td>\n",
       "      <td>[Kada je počelo izdavanje Scholastičkog časopi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Univerzitet je glavna sedište Kongregacije Sve...</td>\n",
       "      <td>[Gde je sedište Kongregacije Svetog Krsta?, Št...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Koledž za inženjerstvo osnovan je 1920. godine...</td>\n",
       "      <td>[Koliko diploma na nivo BS se nudi u Koledžu z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Svi studenti Notr Dame-a su deo jednog od pet ...</td>\n",
       "      <td>[Koje su subjekti koje pružaju pomoć u upravlj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  University_of_Notre_Dame   \n",
       "1  University_of_Notre_Dame   \n",
       "2  University_of_Notre_Dame   \n",
       "3  University_of_Notre_Dame   \n",
       "4  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Arhitektonski, škola ima katolički karakter. N...   \n",
       "1  Kao i na većini drugih univerziteta, studenti ...   \n",
       "2  Univerzitet je glavna sedište Kongregacije Sve...   \n",
       "3  Koledž za inženjerstvo osnovan je 1920. godine...   \n",
       "4  Svi studenti Notr Dame-a su deo jednog od pet ...   \n",
       "\n",
       "                                             queries  \n",
       "0  [Kome se u 1858. godini u Lurdu, Francuska, na...  \n",
       "1  [Kada je počelo izdavanje Scholastičkog časopi...  \n",
       "2  [Gde je sedište Kongregacije Svetog Krsta?, Št...  \n",
       "3  [Koliko diploma na nivo BS se nudi u Koledžu z...  \n",
       "4  [Koje su subjekti koje pružaju pomoć u upravlj...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_table = pq.read_table(\"datasets/dataset_processed.parquet\")\n",
    "df = loaded_table.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selena/sele/SRBedding/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#mead puling\n",
    "def get_sentence_embedding(sentence, model, tokenizer): \n",
    "    \"\"\"\"\"\n",
    "    Pitanja:\n",
    "    - Da li koristiti mean pooling ili CSL pooling?\n",
    "    - U SQuAD datasetu su queries string i treba ih parsirati u listu\n",
    "    \"\"\"\"\"\n",
    "    encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "    attention_mask = encoded_input['attention_mask']   # to indicate which tokens are valid and which are padding\n",
    "    \n",
    "    # Get the model output (without the specific classification head)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    token_embeddings = output.last_hidden_state # poslednji nivo\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    # mean pooling operation, considering the BERT input_mask and padding\n",
    "    sentence_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    return sentence_embedding.flatten().tolist()\n",
    "    # return sentence_embedding\n",
    "\n",
    "\n",
    "def cos_sim(context_embed, query_embed):\n",
    "    return context_embed @ query_embed.T\n",
    "\n",
    "\n",
    "def evaluate(model_name, dataset_path):\n",
    "    # df = pd.read_csv(dataset_path)\n",
    "    # df = pd.read_pickle('dataset_for_eval.pkl') # picke jer je bio problem ucitati df preko pandas. Queries je string i treba parsirati u listu\n",
    "    # with open('dataset_for_eval.pkl', 'rb') as file:\n",
    "    # # Load the pickled data\n",
    "    #     byte_data = file.read()\n",
    "\n",
    "    # # Decode the byte data as UTF-8 if necessary\n",
    "    # # Normally you would not do this step, as pickle handles it\n",
    "    # utf8_data = byte_data.decode('utf-8')\n",
    "\n",
    "    # # Now load it using pickle, if it was necessary to decode (rare)\n",
    "    # df = pickle.loads(utf8_data.encode('utf-8'))\n",
    "    # with open('income.pickle', 'rb') as f:  # with statement avoids file leak\n",
    "    #     # Match load with file object, and provide encoding for Py2 str\n",
    "    #     df = pickle.load(f, encoding='utf-8')\n",
    "\n",
    "    loaded_table = pq.read_table(\"datasets/dataset_processed.parquet\")\n",
    "    df = loaded_table.to_pandas()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    ground_truth = {}# ovo je nase\n",
    "    # qrel = {\n",
    "    #     \"tekst_query1\":\n",
    "    #     {\"1_id_paragrafa\": 0,\n",
    "    #     \"2_id_paragrafa\": 1,\n",
    "    #     \"3_id_paragrafa\": 0},\n",
    "    #     \"tekst_query2\":\n",
    "    #     {\"1_id_paragrafa\": 1,\n",
    "    #     \"2_id_paragrafa\": 0,\n",
    "    #     \"3_id_paragrafa\": 0},\n",
    "    # }\n",
    "    retrieved_scores = {}  # ovo su cosinusne\n",
    "\n",
    "    #we embed the contexts\n",
    "    context_embeddings = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        # if idx >= 2:  # Break the loop after two iterations\n",
    "        #     break\n",
    "        context = row['context']\n",
    "        embedded_context = get_sentence_embedding(context, model, tokenizer)\n",
    "        context_embeddings[str(idx)] = embedded_context\n",
    "        # print(idx)\n",
    "    \n",
    "    # we fill the dict used for metrics\n",
    "    for idx, row in df.iterrows():\n",
    "        # if idx >= 2:  # Break the loop after two iterations\n",
    "        #     break\n",
    "        queries = row['queries'] # [1:-1].split(\"', ' \") zato sto je sacivan kao string pa mora u listu da se pretvara\n",
    "        # print(queries)\n",
    "        # print(type(queries))\n",
    "        for query in queries:\n",
    "            query = query.strip()\n",
    "            # print(query)\n",
    "            ground_truth[query] = {}\n",
    "            retrieved_scores[query] = {}\n",
    "            embedded_query = get_sentence_embedding(query, model, tokenizer)\n",
    "            for current_context_id, current_context_embedding in context_embeddings.items():\n",
    "                ground_truth[query][current_context_id] = 0\n",
    "                # score = cos_sim(current_context_embedding, embedded_query)\n",
    "                # print(embedded_query)\n",
    "                score = cosine_similarity(np.array(current_context_embedding).reshape(1, -1), np.array(embedded_query).reshape(1, -1))\n",
    "                # print(score.item())\n",
    "                retrieved_scores[query][current_context_id] = score.item()\n",
    "            ground_truth[query][str(idx)] = 1 # samo ovaj kome pripada je 1 ostali su nula, necemo da punimo jer je default dict\n",
    "            \n",
    "    \n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "    ground_truth, {'map', 'ndcg'}\n",
    "    ) \n",
    "    # print(context_embeddings)\n",
    "    print(json.dumps(evaluator.evaluate(retrieved_scores), indent=1))\n",
    "    # evaluator.evaluate(run)\n",
    "    return retrieved_scores\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run = evaluate(\"classla/bcms-bertic\", 'datasets/dataset_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kome se u 1858. godini u Lurdu, Francuska, navodno pojavila Deva Marija?': {'0': 0.9702974328351258, '1': 0.9417458116887851}, 'Šta je ispred Glavne zgrade Notr Dame?': {'0': 0.9398622207027303, '1': 0.9148289297375717}, 'Bazilika Svetog srca u Notr Dami je pored koje strukture?': {'0': 0.9571723517991639, '1': 0.9233963285326181}, 'Šta je Grota u Notr Dami?': {'0': 0.9302698055098331, '1': 0.8982230382425597}, 'Šta se nalazi na vrhu Glavne zgrade u Notr Damu?': {'0': 0.97306117336876, '1': 0.9459882890289258}, 'Kada je počelo izdavanje Scholastičkog časopisa Notr dame?': {'0': 0.9430615139247354, '1': 0.909179445095098}, 'Koliko često se \"Notr Dams džongler\" objavljuje?': {'0': 0.9401447997890098, '1': 0.9091041988650693}, 'Kako se zove dnevni studentski list u Notr Damu?': {'0': 0.9681517831548883, '1': 0.9366752558501559}, 'Koliko studentskih novinara se nalazi na Notr Dame?': {'0': 0.9355253300773242, '1': 0.9058199066693698}, 'U kojoj godini je studentski list \"Sporočni razum\" počeo da se objavljuje na Notr Damu?': {'0': 0.9727433045875646, '1': 0.9439486819655618}}\n"
     ]
    }
   ],
   "source": [
    "print(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformer\n",
    "\n",
    "# batching u sentence transformerima i trnasformerima, \n",
    "\n",
    "# embedowan query i array svih conteksta u 1 calling, sortiraju se, top k, dobiju se indexi\n",
    "\n",
    "#clod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
