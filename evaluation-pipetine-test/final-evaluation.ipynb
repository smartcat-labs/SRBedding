{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import openai\n",
    "from openai import APIError\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import AutoModel\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\n",
    "from collections import defaultdict\n",
    "import pytrec_eval\n",
    "import json\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "import CustomInformationRetrievalEvaluator\n",
    "import importlib\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# must for Custom scripts\n",
    "importlib.reload(CustomInformationRetrievalEvaluator)\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jobs(data_pair, filename):    \n",
    "    jobs = [\n",
    "                {\n",
    "                    \"model\": \"text-embedding-3-small\",\n",
    "                    # \"response_format\": \"json\", # TODO check \n",
    "                    # \"temperature\": 0,\n",
    "                    \"metadata\": {\"id\": indx},\n",
    "                    \"input\": text\n",
    "                }\n",
    "                for indx, text in data_pair.items()\n",
    "            ]\n",
    "    with open(filename, \"w\") as f:\n",
    "        for job in jobs:\n",
    "            json_string = json.dumps(job)\n",
    "            f.write(json_string + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# python evaluation-pipetine-test/api_request_parallel_processor.py   --requests_filepath evaluation-pipetine-test/processed_datasets/queries   --save_filepath evaluation-pipetine-test/processed_datasets/example_requests_to_parallel_process_results.jsonl   --request_url https://api.openai.com/v1/embeddings   --max_requests_per_minute 1500   --max_tokens_per_minute 6250000   --token_encoding_name cl100k_base   --max_attempts 5   --logging_level 20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/dataset_processed.parquet\n",
      "text-embedding-3-small\n",
      "True\n",
      "{'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@1': 0.8620689655172413, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@3': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@5': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@10': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@3': np.float64(0.3333333333333334), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@5': np.float64(0.20000000000000007), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@10': np.float64(0.10000000000000003), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@3': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@5': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@10': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_ndcg@10': np.float64(0.9445789400246336), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_mrr@10': 0.9252873563218391, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_map@100': np.float64(0.925287356321839), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@1': 0.8620689655172413, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@3': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@5': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@10': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@3': np.float64(0.3333333333333334), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@5': np.float64(0.20000000000000007), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@10': np.float64(0.10000000000000003), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@3': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@5': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@10': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_ndcg@10': np.float64(0.9445789400246336), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_mrr@10': 0.9252873563218391, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_map@100': np.float64(0.925287356321839)}\n"
     ]
    }
   ],
   "source": [
    "def get_data_for_evaluation(dataset_name):\n",
    "    loaded_table = pq.read_table(dataset_name)\n",
    "    df = loaded_table.to_pandas()\n",
    "    corpus = {}\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    query_idx = 1\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx >= 6:  # Break the loop after two iterations\n",
    "            break\n",
    "        corpus[idx] = row['context']\n",
    "        for query in row['queries']:\n",
    "            query = query.strip()\n",
    "            queries[query_idx] = query\n",
    "            if query_idx not in relevant_docs:\n",
    "                relevant_docs[query_idx] = set()\n",
    "            relevant_docs[query_idx].add(idx)\n",
    "            query_idx += 1\n",
    "    save_jobs(queries, \"processed_datasets/queries\")\n",
    "    return queries, corpus, relevant_docs\n",
    "\n",
    "def load_sentence_tranformer_from_transformer(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # Combine the model and pooling into a SentenceTransformer\n",
    "    word_embedding_model = models.Transformer(model_name_or_path=model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_dimension=model.config.hidden_size, pooling_mode_mean_tokens=True)\n",
    "    return SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "def get_model_or_model_name(model_name, is_openAI):\n",
    "    model = None\n",
    "    if not is_openAI:\n",
    "        model = load_sentence_tranformer_from_transformer(model_name)\n",
    "        model_name = None\n",
    "    return model_name,model\n",
    "\n",
    "def evaluate(model_name, dataset_name, is_openAI):\n",
    "\n",
    "    name = f\"{model_name}-{dataset_name}-evlatuation\"\n",
    "    queries, corpus, relevant_docs = get_data_for_evaluation(dataset_name)\n",
    "\n",
    "    ir_evaluator = CustomInformationRetrievalEvaluator.InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=name,\n",
    "    write_csv=True\n",
    "    )\n",
    "\n",
    "    model_name, model = get_model_or_model_name(model_name, is_openAI)\n",
    "        \n",
    "    results = ir_evaluator(model, openAI_model=model_name)\n",
    "    # print(ir_evaluator.primary_metric)\n",
    "    # print(results[ir_evaluator.primary_metric])\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"datasets/dataset_processed.parquet\"]\n",
    "\n",
    "models_ = {\n",
    "#  \"google-bert/bert-base-multilingual-cased\": False\n",
    " \"text-embedding-3-small\" : True   \n",
    "}\n",
    "\n",
    "for dataset_name, model_name in zip(datasets, models_.keys()):\n",
    "    print(dataset_name)\n",
    "    print(model_name)\n",
    "    print(models_[model_name])\n",
    "    print(evaluate(model_name=model_name, dataset_name=dataset_name, is_openAI=models_[model_name]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@1': 0.8620689655172413,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@10': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@3': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@5': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_map@100': np.float64(0.925287356321839),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_mrr@10': 0.9252873563218391,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_ndcg@10': np.float64(0.9445789400246336),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@1': np.float64(0.8620689655172413),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@10': np.float64(0.10000000000000003),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@3': np.float64(0.3333333333333334),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@5': np.float64(0.20000000000000007),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@1': np.float64(0.8620689655172413),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@10': np.float64(1.0),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@3': np.float64(1.0),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@5': np.float64(1.0),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@1': 0.8620689655172413,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@10': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@3': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@5': 1.0,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_map@100': np.float64(0.925287356321839),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_mrr@10': 0.9252873563218391,\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_ndcg@10': np.float64(0.9445789400246336),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@1': np.float64(0.8620689655172413),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@10': np.float64(0.10000000000000003),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@3': np.float64(0.3333333333333334),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@5': np.float64(0.20000000000000007),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@1': np.float64(0.8620689655172413),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@10': np.float64(1.0),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@3': np.float64(1.0),\n",
      " 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@5': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "pprint({'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@1': 0.8620689655172413, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@3': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@5': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_accuracy@10': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@3': np.float64(0.3333333333333334), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@5': np.float64(0.20000000000000007), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_precision@10': np.float64(0.10000000000000003), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@3': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@5': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_recall@10': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_ndcg@10': np.float64(0.9445789400246336), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_mrr@10': 0.9252873563218391, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_cosine_map@100': np.float64(0.925287356321839), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@1': 0.8620689655172413, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@3': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@5': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_accuracy@10': 1.0, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@3': np.float64(0.3333333333333334), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@5': np.float64(0.20000000000000007), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_precision@10': np.float64(0.10000000000000003), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@1': np.float64(0.8620689655172413), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@3': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@5': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_recall@10': np.float64(1.0), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_ndcg@10': np.float64(0.9445789400246336), 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_mrr@10': 0.9252873563218391, 'text-embedding-3-small-datasets/dataset_processed.parquet-evlatuation_dot_map@100': np.float64(0.925287356321839)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
