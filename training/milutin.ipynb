{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas\n",
    "import pyarrow.parquet as pq\n",
    "import sentence_transformers.losses as losses\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    models,\n",
    ")\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModel,\n",
    "    QuantoConfig,\n",
    ")\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from quanto import quantize, freeze, qint8, qfloat8\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_transformer(\n",
    "    model_name: str, max_seq_length: int = 512\n",
    ") -> SentenceTransformer:\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # tokenizer.model_max_length = max_seq_length  # Set the max length for the model\n",
    "    # tokenizer.padding_side = (\n",
    "    #     \"right\"  # You can set \"left\" if you want to pad on the left side\n",
    "    # )\n",
    "    # # tokenizer.pad_token = tokenizer.eos_token  # Ensure the pad token is set\n",
    "    # model = SentenceTransformer(model_name)\n",
    "    # # Add the padding and truncation to the encode method\n",
    "    # model.tokenizer = tokenizer\n",
    "    # model.tokenizer_kwargs = {\n",
    "    #     \"padding\": \"max_length\",\n",
    "    #     \"truncation\": True,\n",
    "    #     \"max_length\": max_seq_length,\n",
    "    #     \"return_tensors\": \"pt\",  # Assuming you want PyTorch tensors as output\n",
    "    # }\n",
    "    # return model\n",
    "\n",
    "\n",
    "    # Load model directly\n",
    "    # quantization_config = BitsAndBytesConfig(load_in_8bit = True)\n",
    "    # quantization_config = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "    # word_embedding_model = AutoModel.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "\n",
    "    # # Apply mean pooling to get one fixed sized sentence vector\n",
    "    # pooling_model = models.Pooling(max_seq_length,\n",
    "    #                             pooling_mode_cls_token=False,\n",
    "    #                             pooling_mode_max_tokens=False,\n",
    "    #                             pooling_mode_mean_tokens=True)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # tokenizer.model_max_length = max_seq_length  # Set the max length for the model\n",
    "    # tokenizer.padding_side = (\n",
    "    #     \"right\"  # You can set \"left\" if you want to pad on the left side\n",
    "    # )\n",
    "    # # tokenizer.pad_token = tokenizer.eos_token  # Ensure the pad token is set\n",
    "    # model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    # # Add the padding and truncation to the encode method\n",
    "    # model.tokenizer = tokenizer\n",
    "    # model.tokenizer_kwargs = {\n",
    "    #     \"padding\": \"max_length\",\n",
    "    #     \"truncation\": True,\n",
    "    #     \"max_length\": max_seq_length,\n",
    "    #     \"return_tensors\": \"pt\",  # Assuming you want PyTorch tensors as output\n",
    "    # }\n",
    "    # return model\n",
    "\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "    # Apply mean pooling to get one fixed sized sentence vector\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_cls_token=False,\n",
    "        pooling_mode_max_tokens=False,\n",
    "        pooling_mode_mean_tokens=True,\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    # quantize(model, weights=qfloat8, activations=qfloat8)\n",
    "    # freeze(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at jerteh/Jerteh-355 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/selena/sele/SRBedding/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n"
     ]
    }
   ],
   "source": [
    "jerteh = make_sentence_transformer(\"jerteh/Jerteh-355\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jerteh.half()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
