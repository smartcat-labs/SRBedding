{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas\n",
    "import openai\n",
    "from openai import APIError\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer,  models, util\n",
    "from sentence_transformers.readers import InputExample\n",
    "from enum import Enum\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import math\n",
    "import sentence_transformers.losses  as losses\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(file: Path) -> pandas.DataFrame:\n",
    "    loaded_table = pq.read_table(file)\n",
    "    return loaded_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryType(Enum):\n",
    "    SHORT = 'short_query'\n",
    "    MEDIUM = 'medium_query'\n",
    "    LONG = 'long_query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_dataset(dataframe: pandas.DataFrame, question_type: str) -> List[InputExample]:\n",
    "    dataset_samples = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        score = float(row['scores'][question_type]) / 5.0\n",
    "        sample = InputExample(texts=[row['context'], row[question_type]],\n",
    "                                 label=score)\n",
    "        dataset_samples.append(sample)\n",
    "    return dataset_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hf_dataset(input_examples):\n",
    "    # Convert each InputExample into a dictionary\n",
    "    data_dict = {\n",
    "        \"sentence1\": [ex.texts[0] for ex in input_examples],\n",
    "        \"sentence2\": [ex.texts[1] for ex in input_examples],\n",
    "        \"score\": [ex.label for ex in input_examples]\n",
    "    }\n",
    "    \n",
    "    # Create a Hugging Face Dataset\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def get_train_and_eval_datasets():\n",
    "    df = load_df(file=Path(\"datasets/train.parquet\"))\n",
    "    training_samples = convert_dataset(df, QueryType.LONG.value)\n",
    "\n",
    "    # Manually split the dataset while retaining the original structure\n",
    "    dataset_size = len(training_samples)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "\n",
    "    train_samples = training_samples[:train_size]\n",
    "    eval_samples = training_samples[train_size:]\n",
    "\n",
    "    # Convert lists to Hugging Face Datasets\n",
    "    train_dataset = convert_to_hf_dataset(train_samples)\n",
    "    eval_dataset = convert_to_hf_dataset(eval_samples)\n",
    "\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_transformer(model_name :str) -> SentenceTransformer:\n",
    "    max_seq_length = 128\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "    # Apply mean pooling to get one fixed sized sentence vector\n",
    "    \"\"\"Performs pooling (max or mean) on the token embeddings.\n",
    "    Iit generates from a variable sized sentence a fixed sized sentence embedding, \n",
    "    allows to use the CLS token if it is returned by the underlying word embedding model.\n",
    "    We can concatenate multiple poolings together.\n",
    "    - word_embedding_dimension: Dimensions for the word embeddings\n",
    "    - pooling_mode_cls_token: Use the first token (CLS token) as text representations\n",
    "    - pooling_mode_max_tokens: Use max in each dimension over all tokens.\n",
    "    - pooling_mode_mean_tokens: Perform mean-pooling\n",
    "    \"\"\"\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                                pooling_mode_cls_token=False,\n",
    "                                pooling_mode_max_tokens=False,\n",
    "                                pooling_mode_mean_tokens=True)\n",
    "    return SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nesto\"\n",
    "model_save_path = 'output/training_stsbenchmark_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "        # Required parameter:\n",
    "        output_dir=model_save_path,\n",
    "        # Optional training parameters:\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "        bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "        # batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "        # Optional tracking/debugging parameters:\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        run_name=\"proba\",  # Will be used in W&B if `wandb` is installed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = get_train_and_eval_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Za izvršavanje programa koje pišemo na Pajtonu, potreban nam je program koji se zove Pajton interpreter. Ovaj program tumači (interpretira), a zatim i izvršava Pajton naredbe. Pajton interpreteri mogu da prihvate cele programe i da ih izvrše, a mogu da rade i u interaktivnom režimu, ', 'sentence2': 'Koja je uloga Pajton interpretera u izvršavanju programa na Pajtonu i na koji način tumači Pajton naredbe?', 'score': 0.8}\n",
      "{'sentence1': 'Sredinom 6. veka, područje naseljavaju Sloveni. U 9. veku, tvrđava Petrikon (ili na slovenskim jezicima - Petrik) ulazi u sastav Bugarskog carstva, a u 11. veku njom vlada sremski vojvoda Sermon, čiji su zlatnici u 19. veku pronađeni u jednom petrovaradinskom vinogradu. Pošto je Bugarska poražena od Vizantije a Sermon ubijen, tvrđava ponovo postaje deo Vizantije, da bi, posle borbi Vizantinaca i Mađara, krajem 12. veka, ušla u sastav srednjovekovne Kraljevine Ugarske. Na području Bačke, ugarska vlast se ustaljuje nešto ranije, tokom 10. veka.', 'sentence2': 'Kako je tvrđava Petrikon menjala vlasnike od 6. do 12. veka i koje su ključne bitke obeležile tu promenu vlasti?', 'score': 0.6}\n",
      "{'sentence1': 'U vreme Osmanske uprave poznat pod imenom Varadin, Petrovaradin je bio sedište nahije u okviru Sremskog sandžaka. Podgrađe tvrđave imalo je oko 200 kuća, tu se nalazila Sulejman-hanova džamija, a postojale su i dve manje džamije, Hadži-Ibrahimova i Huseinova. Pored dve turske mahale, u sastavu grada nalazila se i hrišćanska četvrt sa 35 kuća, naseljenih isključivo Srbima.[8] Od praistorije pa sve do kraja 17. veka, centar urbanog života na području današnjeg grada nalazio se na sremskoj strani Dunava, na prostoru današnjeg Petrovaradina, koji je svojim značajem uvek zasenjivao naselja na bačkoj strani.', 'sentence2': 'Koliko kuća je bilo u hrišćanskoj četvrti u Petrovaradinu, naseljenoj isključivo Srbima?', 'score': 0.4}\n",
      "{'sentence1': 'Novi Sad je Evropska prestonica kulture 2022. Novi Sad je, posle Beograda, drugi grad u Srbiji po broju stanovnika (bez podataka za područje Kosova i Metohije). Na poslednjem zvaničnom popisu iz 2011. godine, sam grad je imao 231.798[2] stanovnika. Na opštinskom području Novog Sada (uključujući i prigradska nasenja) broj stanovnika je 2011. godine iznosio 341.625.[', 'sentence2': 'Koje je godine Novi Sad proglašen za Evropsku prestonicu kulture i koliko je stanovnika imao prema poslednjem zvaničnom popisu iz 2011. godine?', 'score': 0.6}\n",
      "{'sentence1': ' Novom Sadu je, tokom celog rata, delovao pokret otpora i oslobodilački pokret pod vođstvom Komunističke Partije Jugoslavije. U gradu se nalazilo sedište okružnog komiteta Komunističke Partije Jugoslavije, koji je u okviru narodnooslobodilačkog pokreta bio deo Pokrajinskog komiteta KPJ za Vojvodinu. U narodnooslobodilačkoj borbi, u partizanskim odredima i vojvođanskim brigadama, neposredno je učestvovalo 2.365 Novosađana, pripadnika svih nacionalnosti, (Srba, Mađara, Slovaka i ostalih)', 'sentence2': 'Ko je bio na čelu okružnog komiteta Komunističke Partije Jugoslavije u Novom Sadu tokom rata?', 'score': 0.8}\n",
      "{'sentence1': 'Još jedan način da pokrenete Pajton školjku je da otvorite komandni prozor (na Windows sistemima to se radi pokretanjem programa cmd), a zatim u komandnom prozoru otkucate Python (ovde podrazumevamo da je Pajton instaliran tako da je dostupan iz svakog foldera, u protivnom treba se prvo pozicionirati u folder u kome se nalazi Pajton interpreter).', 'sentence2': 'Kako se može pokrenuti Pajton školjka putem komandnog prozora na Windows sistemima ako je Pajton dostupan iz svakog foldera?', 'score': 0.8}\n",
      "{'sentence1': 'Pajton je veoma popularan programski jezik opšte namene. Postao je poznat po svojoj jednostavnosti, lakoći učenja i brzini programiranja. Mnogi profesionalni programeri koriste Pajton bar kao pomoćni jezik, jer pomoću njega brzo i lako automatizuju razne poslove. ', 'sentence2': 'Koje su karakteristike Pajtona zbog kojih je postao popularan među programerima i kako ga koriste u automatizaciji poslova?', 'score': 0.8}\n",
      "{'sentence1': 'Najstariji arheološki ostaci (iz vremena kamenog doba) pronađeni su sa obe strane Dunava, na području današnjeg Petrovaradina (koji je u kontinuitetu nastanjen od praistorije do danas) i području današnje Klise. Istraživanjem ostataka naselja iz mlađeg bronzanog doba (3000. godina pne.) na području današnjeg Petrovaradina, arheolozi su pronašli i bedeme pojačane koljem i palisadama iz tog perioda, koji svedoče da je još u vreme vučedolske kulture ovde postojalo utvrđeno naselje.', 'sentence2': 'Koje su specifične strukture pronađene na području Petrovaradina koje svedoče o postojanju utvrđenog naselja u vreme vučedolske kulture?', 'score': 0.4}\n"
     ]
    }
   ],
   "source": [
    "for s in train_dataset:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_model(model_name:str, args: SentenceTransformerTrainingArguments):\n",
    "    num_epochs = 10\n",
    "    train_dataset, eval_dataset = get_train_and_eval_datasets()\n",
    "    sentence_transformer = make_sentence_transformer(model_name)\n",
    "    warmup_steps = math.ceil(len(train_dataset) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "    train_loss = losses.CosineSimilarityLoss(model=sentence_transformer)\n",
    "    train_loss = losses.MatryoshkaLoss(sentence_transformer, train_loss, [768, 512, 256, 128, 64])\n",
    "  \n",
    "\n",
    "    sentences1 = [sample['sentence1'] for sample in eval_dataset]\n",
    "    sentences2 = [sample['sentence2'] for sample in eval_dataset]\n",
    "    scores = [sample['score'] for sample in eval_dataset]\n",
    "    print(f'duzina {len(sentences1)}')\n",
    "    # # 6. (Optional) Create an evaluator & evaluate the base model\n",
    "    dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=sentences1,\n",
    "        sentences2=sentences2,\n",
    "        scores=scores,\n",
    "        main_similarity=SimilarityFunction.COSINE,\n",
    "        name=\"sts-dev\",\n",
    "    )\n",
    "\n",
    "    dev_evaluator(sentence_transformer)\n",
    "\n",
    "    # 7. Create a trainer & train\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=sentence_transformer,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        loss=train_loss,\n",
    "        evaluator=dev_evaluator,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # # (Optional) Evaluate the trained model on the test set\n",
    "    test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=eval_dataset[\"sentence1\"],\n",
    "        sentences2=eval_dataset[\"sentence2\"],\n",
    "        scores=eval_dataset[\"score\"],\n",
    "        main_similarity=SimilarityFunction.COSINE,\n",
    "        name=\"sts-dev\",\n",
    "    )\n",
    "    test_evaluator(sentence_transformer)\n",
    "\n",
    "    # 8. Save the trained model\n",
    "    sentence_transformer.save_pretrained(\"output/mpnet-base-all-nli-triplet/final\")\n",
    "\n",
    "    # 9. (Optional) Push it to the Hugging Face Hub\n",
    "    # model.push_to_hub(\"mpnet-base-all-nli-triplet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/home/selena/sele/SRBedding/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duzina 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9206eb2e98b441f7baac697c15344fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0113378da7b24753855aa5fc0635b9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4ec96a9204bfcbc3d80d238aa1840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.0678, 'train_samples_per_second': 1.318, 'train_steps_per_second': 0.165, 'train_loss': 0.18574048578739166, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719874be3faf453390101e93693175de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04934d0ea41447e91ffe9f2e1a82f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_a_model(\"google-bert/bert-base-multilingual-cased\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gold_samle(df_train):\n",
    "    gold_samples = []\n",
    "    batch_size = 16\n",
    "    for df in df_train:\n",
    "        score = float(df['score'])/5.0  # Normalize score to range 0 ... 1\n",
    "        gold_samples.append(InputExample(texts=[df['sentence1'], df['sentence2']], label=score))\n",
    "        gold_samples.append(InputExample(texts=[df['sentence2'], df['sentence1']], label=score))\n",
    "    # We wrap gold_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "    return DataLoader(gold_samples, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder_path = 'output/cross-encoder/stsb_indomain_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "###### Cross-encoder (simpletransformers) ######\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\n",
    "cross_encoder = CrossEncoder(model_name, num_labels=1)\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(eval_dataset, name='sts-dev')\n",
    "train_dataloader = make_gold_samle(train_dataset)\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srbedding-4dwWae5r-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
