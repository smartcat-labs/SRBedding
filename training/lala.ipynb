{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\smartCat\\SRBedding\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas\n",
    "import pyarrow.parquet as pq\n",
    "import sentence_transformers.losses as losses\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    models\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    SimilarityFunction,\n",
    ")\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TrainerCallback, TrainerControl, TrainerState\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class QueryType(Enum):\n",
    "    SHORT = \"short_query\"\n",
    "    MEDIUM = \"medium_query\"\n",
    "    LONG = \"long_query\"\n",
    "\n",
    "\n",
    "def load_df(file: Path) -> pandas.DataFrame:\n",
    "    loaded_table = pq.read_table(file)\n",
    "    return loaded_table.to_pandas()\n",
    "\n",
    "\n",
    "def convert_to_hf_dataset(dataframe: pandas.DataFrame, question_type:str) -> Dataset:\n",
    "    # Convert each InputExample into a dictionary\n",
    "    data_dict = {\n",
    "        \"anchor\": [],\n",
    "        \"positive\": [],\n",
    "    }\n",
    "    for _, row in dataframe.iterrows():\n",
    "        data_dict['anchor'].append(row[question_type])\n",
    "        data_dict['positive'].append(row['context'])\n",
    "    # Create a Hugging Face Dataset\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "def get_train_and_eval_datasets(\n",
    "    dataset_name: Path,\n",
    "    question_type:str\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    # NOTE francuzi su 70:15:15 ovde je 80:10:10\n",
    "    df = load_df(file=dataset_name)\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    dataset_counts_train = train_df['dataset'].value_counts()\n",
    "    dataset_counts_eval = eval_df['dataset'].value_counts()\n",
    "    # Convert lists to Hugging Face Datasets\n",
    "    train_dataset = convert_to_hf_dataset(train_df, question_type)\n",
    "    eval_dataset = convert_to_hf_dataset(eval_df, question_type)\n",
    "\n",
    "    return train_dataset, eval_dataset, dataset_counts_train, dataset_counts_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_table = pq.read_table('datasets/wiki.parquet')\n",
    "df_wiki = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/science.parquet')\n",
    "df_science = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/news.parquet')\n",
    "df_news = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/literature.parquet')\n",
    "df_literature = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/wiki_fixed.parquet')\n",
    "df_wiki_fixed = loaded_table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['dataset'] = 'wiki'\n",
    "df_wiki_fixed['dataset'] = 'wiki'\n",
    "df_science['dataset'] = 'science'\n",
    "df_news['dataset'] = 'news'\n",
    "df_literature['dataset'] = 'literature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pandas.concat([df_wiki_fixed, df_science, df_news, df_literature], ignore_index=True)\n",
    "# Optionally, you can save the merged dataframe to a file\n",
    "merged_df.to_parquet('datasets/TRAIN11k_fixed_v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "wiki          7444\n",
      "science       2095\n",
      "news          1750\n",
      "literature     425\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_counts = merged_df['dataset'].value_counts()\n",
    "print(dataset_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ev, dataset_counts_train, dataset_counts_eval = get_train_and_eval_datasets(dataset_name=\"datasets/TRAIN11k_fixed_v2.parquet\", question_type=\"short_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "wiki          0.632590\n",
      "science       0.180130\n",
      "news          0.150464\n",
      "literature    0.036816\n",
      "Name: count, dtype: float64\n",
      "dataset\n",
      "wiki          0.647034\n",
      "science       0.173709\n",
      "news          0.145113\n",
      "literature    0.034144\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dataset_proportions = dataset_counts_train / dataset_counts_train.sum()\n",
    "\n",
    "# Print the proportions\n",
    "print(dataset_proportions)\n",
    "\n",
    "dataset_proportions = dataset_counts_eval / dataset_counts_eval.sum()\n",
    "\n",
    "# Print the proportions\n",
    "print(dataset_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive'],\n",
       "    num_rows: 9371\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Šta je uticalo na loše rezultate poslovanja?'\n",
      "('319 stabilno poslovanje Drugi stratum čini takođe šest velikih privrednih '\n",
      " 'subjekata koji su u istom vremenskom okviru imali narušenu finansijsku '\n",
      " 'strukturu, što se direktno odrazilo i na loše rezutate poslovanja Na bazi '\n",
      " 'predloženih kompanija formirane su dve polarizovane grupe velikih privrednih '\n",
      " 'subjekata kako bi putem predloženog modela mogla da se pokaže homogenost u '\n",
      " 'kretanju kvantitativnih i kvalitativnih pokazatelja Takođe, polarizovani '\n",
      " 'pristup u istraživanju dao je mogućnost dodatnog testiranja pouzdanosti '\n",
      " 'predloženog modela ocene kreditnog boniteta velikih privrednih subjekata')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(tr['anchor'][2])\n",
    "pprint(tr['positive'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'score'],\n",
       "    num_rows: 1128\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jerteh/Jerteh-355 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length for the model is: 514\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load the model\n",
    "model_name = \"jerteh/Jerteh-355\"  # Replace this with the correct model identifier\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model = model.half()\n",
    "# Access the max sequence length\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "\n",
    "print(f\"The maximum sequence length for the model is: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_table = pq.read_table('datasets/TRAIN11k_fixed_v2.parquet')\n",
    "train = loaded_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonizacija definicija grada - ESPON 2013 Koncepti evropskih statističkih jedinica analize (NUTS; LAU), urbanih morfoloških područja funkcionalnih urbanih područja (FUA) prihvaćeni su, sinhronizovani i dalje razvijani u sklopu projekata ESPON 2013 Database (ESPON, 2013d; ESPON Database 2013, 2014; ESPON, 2013c) Evropske posmatračke mreže za teritorijalni razvoj i koheziju (ESPON - European Observation Network for Territorial Development and Cohesion) Baza podataka koja je nastala kao rezultat ovog projekta dalje se razvila kroz ESPON M4D (Multi Dimension Database Design and Development) projekat, koji za glavni cilj ima održavanje, ažuriranje, razvoj i proširenje baze podataka ESPON 2013 (ESPON, ESPON baza pruža temeljne regionalne informacije obezbjeđene iz ESPON-ovih projekata i od strane Eurostata, a koje se mogu koristiti kao podrška za teritorijalnu analizu razvoja na različitim geografskim nivoima\n",
      "Od 1874. do 1882. godine, pohađao je gimnaziju. Za to vreme, napisao je svoje prve kompozicije, prva kamerna dela. Njegovo školovanje, međutim, bilo je pod strogim očevim nadzorom i uglavnom ograničeno na dela klasičnih kompozitora: Hajdna, Mocarta i Betovena. Tek u šesnaestoj godini je Rihard uspeo da se, uprkos očevom negodovanju, dočepa partiture Vagnerovog \"Tristana\", koji mu je istinski otvorio uši za snagu muzike. Zanimljivo je, dakle, da je Štraus tako brzo uspeo da se otrese od oca nasleđene mržnje prema Vagneru (ranije je za \"Zigfrida\" rekao: \"Tu nema ni traga koherentne melodije... potpuni haos, rekao bih\")..\n",
      " Iz Nikšića je i Filip Vučić, jedini predstavnik državne zajednice Srbija i Crna Gora na evroviziji za djecu Nikšić, kao izuzetno inspirativna sredina, nosi epitet grada velikih umjetnika i zvučnih imena kulturnih poslenika U oblasti slikarstva nezaobilazno ime je Vojo Stanić, čuveni jugoslovenski i crnogorski slikar, koji je u Nikšiću živio do završetka gimnazije i koji je rekao da mu je život u ovom gradu odredio slikarski stil i koji je uvijek isticao da sebe prije svega smatra Nikšićaninom\n",
      " Dobijen je niz polusintetskih penicilina, ali značajniju ulogu u kliničkom okruženju ima oko desetak predstavnika Dole je dat nepotpun spisak polusintetskih penicilina, grupisanih po izvesnim zajedničkim osobinama Ovo su penicilini proširenog spektra, prigodni za \"-{per os}-\" primenu, ali osetljivi na dejstvo penicilaza Rastvorne soli mogu se primenjivati i parenteralno, kod težih infekcija Sa pojavom sojeva koji luče beta-laktamaze, terapijska vrednost pojedinih penicilina znatno je smanjena jer više nisu pokazivali dobre rezultate protiv intrinsički podložnih bakterija\n",
      " cit, str 130 U slučaju da izvršni poverilac ne uspe da dokaže javnom ili po zakonu overenom ispravom da je ispunio svoju obavezu ili da je uslov nastupio, bio bi u obavezi da pokrene parnični postupak radi utvrđenja da je na osnovu izvršne isprave ovlašćen tražiti i zahtevati bezuslovno izvršenje radi ostvarenja svog potraživanja Radi se o pravu utvrđenja svojevrsnog izvršnog pravozaštitnog zahteva kao javnopravnog ovlašćenja na izvršnopravnu zaštitu – svojevrsna purifikacijska tužba\n"
     ]
    }
   ],
   "source": [
    "for context in train.sample(5)['context']:\n",
    "    print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
