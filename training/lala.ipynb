{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selena/sele/SRBedding/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas\n",
    "import pyarrow.parquet as pq\n",
    "import sentence_transformers.losses as losses\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    models\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    SimilarityFunction,\n",
    ")\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TrainerCallback, TrainerControl, TrainerState\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class QueryType(Enum):\n",
    "    SHORT = \"short_query\"\n",
    "    MEDIUM = \"medium_query\"\n",
    "    LONG = \"long_query\"\n",
    "\n",
    "\n",
    "def load_df(file: Path) -> pandas.DataFrame:\n",
    "    loaded_table = pq.read_table(file)\n",
    "    return loaded_table.to_pandas()\n",
    "\n",
    "\n",
    "def convert_to_hf_dataset(dataframe: pandas.DataFrame, question_type:str) -> Dataset:\n",
    "    # Convert each InputExample into a dictionary\n",
    "    data_dict = {\n",
    "        \"anchor\": [],\n",
    "        \"positive\": [],\n",
    "    }\n",
    "    for _, row in dataframe.iterrows():\n",
    "        data_dict['anchor'].append(row[question_type])\n",
    "        data_dict['positive'].append(row['context'])\n",
    "    # Create a Hugging Face Dataset\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "def get_train_and_eval_datasets(\n",
    "    dataset_name: Path,\n",
    "    question_type:str\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    # NOTE francuzi su 70:15:15 ovde je 80:10:10\n",
    "    df = load_df(file=dataset_name)\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    dataset_counts_train = train_df['dataset'].value_counts()\n",
    "    dataset_counts_eval = eval_df['dataset'].value_counts()\n",
    "    # Convert lists to Hugging Face Datasets\n",
    "    train_dataset = convert_to_hf_dataset(train_df, question_type)\n",
    "    eval_dataset = convert_to_hf_dataset(eval_df, question_type)\n",
    "\n",
    "    return train_dataset, eval_dataset, dataset_counts_train, dataset_counts_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_table = pq.read_table('datasets/wiki_4o.parquet')\n",
    "df_wiki = loaded_table.to_pandas()\n",
    "\n",
    "# loaded_table = pq.read_table('datasets/science.parquet')\n",
    "# df_science = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/news_4o.parquet')\n",
    "df_news = loaded_table.to_pandas()\n",
    "\n",
    "loaded_table = pq.read_table('datasets/literature_4o.parquet')\n",
    "df_literature = loaded_table.to_pandas()\n",
    "\n",
    "# loaded_table = pq.read_table('datasets/wiki_4o.parquet')\n",
    "# df_wiki_fixed = loaded_table.to_pandas()\n",
    "loaded_table = pq.read_table('datasets/TRAIN11k_fixed_v2.parquet')\n",
    "df_11k = loaded_table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>short_query</th>\n",
       "      <th>medium_query</th>\n",
       "      <th>long_query</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UTF-8 UTF-8 varijanta je najzgodnija za kodira...</td>\n",
       "      <td>UTF-8 kodiranje</td>\n",
       "      <td>Unicode fontovi za prenosivost teksta</td>\n",
       "      <td>Kako koristiti UTF-8 u Microsoft Word-u?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Osnovna kodna strana na personalnim računarima...</td>\n",
       "      <td>Kodna strana PC437</td>\n",
       "      <td>Podrška za jezike kodne strane</td>\n",
       "      <td>Kako Unicode rešava problem kodnih strana?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Njen naziv je UCS-2 zato što koristi dva oktet...</td>\n",
       "      <td>UCS-2 karakteri</td>\n",
       "      <td>UTF-16 i problem prostora</td>\n",
       "      <td>Kako UTF-8 rešava problem prenosa podataka?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ova transformaciona šema je prevashodno zgodna...</td>\n",
       "      <td>transformaciona šema latinica</td>\n",
       "      <td>MIME standardi za prenos fajlova</td>\n",
       "      <td>Kako funkcioniše 7-bitna transformaciona šema ...</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Osnovna razlika je u tome što UTF-7 koristi sa...</td>\n",
       "      <td>UTF-7 Base64 prednosti</td>\n",
       "      <td>UCS-4 Unicode standard karakteristike</td>\n",
       "      <td>Zašto je UCS-4 plan za budućnost Unicode-a?</td>\n",
       "      <td>{'long_query': 4, 'medium_query': 4, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>Ova igra, u kojoj nema surovosti, veoma je zg...</td>\n",
       "      <td>pravila rekreativne košarke</td>\n",
       "      <td>pravila takmičarske košarke u NBA</td>\n",
       "      <td>koji su uslovi za takmičarsku košarku</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Većina terena ima parket izrađen od drveta Že...</td>\n",
       "      <td>visina obruča koša</td>\n",
       "      <td>dimenzije terena za košarku</td>\n",
       "      <td>razlika u liniji za 3 poena NBA i međunarodna</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>Igrači su uniformisani i svi iz istog tima nos...</td>\n",
       "      <td>dresovi košarkaša</td>\n",
       "      <td>brojevi na dresovima košarkaša</td>\n",
       "      <td>koliko igrača sme biti na terenu</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Zamene se mogu obavljati neograničeno, ali sam...</td>\n",
       "      <td>zamene sat zaustavljen</td>\n",
       "      <td>koliko traju četvrtine u NBA</td>\n",
       "      <td>koliko traju poluvremena na fakultetima</td>\n",
       "      <td>{'long_query': 4, 'medium_query': 5, 'short_qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>Ako je igra neodlučena nakon četiri četvrtine...</td>\n",
       "      <td>trajanje košarkaške utakmice</td>\n",
       "      <td>koliko traje košarkaška utakmica sa produžecima</td>\n",
       "      <td>koliko produžetaka može biti u košarkaškoj uta...</td>\n",
       "      <td>{'long_query': 4, 'medium_query': 5, 'short_qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>664 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  \\\n",
       "0    UTF-8 UTF-8 varijanta je najzgodnija za kodira...   \n",
       "1    Osnovna kodna strana na personalnim računarima...   \n",
       "2    Njen naziv je UCS-2 zato što koristi dva oktet...   \n",
       "3    Ova transformaciona šema je prevashodno zgodna...   \n",
       "4    Osnovna razlika je u tome što UTF-7 koristi sa...   \n",
       "..                                                 ...   \n",
       "659   Ova igra, u kojoj nema surovosti, veoma je zg...   \n",
       "660   Većina terena ima parket izrađen od drveta Že...   \n",
       "661  Igrači su uniformisani i svi iz istog tima nos...   \n",
       "662  Zamene se mogu obavljati neograničeno, ali sam...   \n",
       "663   Ako je igra neodlučena nakon četiri četvrtine...   \n",
       "\n",
       "                       short_query  \\\n",
       "0                  UTF-8 kodiranje   \n",
       "1               Kodna strana PC437   \n",
       "2                  UCS-2 karakteri   \n",
       "3    transformaciona šema latinica   \n",
       "4           UTF-7 Base64 prednosti   \n",
       "..                             ...   \n",
       "659    pravila rekreativne košarke   \n",
       "660             visina obruča koša   \n",
       "661              dresovi košarkaša   \n",
       "662         zamene sat zaustavljen   \n",
       "663   trajanje košarkaške utakmice   \n",
       "\n",
       "                                        medium_query  \\\n",
       "0              Unicode fontovi za prenosivost teksta   \n",
       "1                     Podrška za jezike kodne strane   \n",
       "2                          UTF-16 i problem prostora   \n",
       "3                   MIME standardi za prenos fajlova   \n",
       "4              UCS-4 Unicode standard karakteristike   \n",
       "..                                               ...   \n",
       "659                pravila takmičarske košarke u NBA   \n",
       "660                      dimenzije terena za košarku   \n",
       "661                   brojevi na dresovima košarkaša   \n",
       "662                     koliko traju četvrtine u NBA   \n",
       "663  koliko traje košarkaška utakmica sa produžecima   \n",
       "\n",
       "                                            long_query  \\\n",
       "0             Kako koristiti UTF-8 u Microsoft Word-u?   \n",
       "1           Kako Unicode rešava problem kodnih strana?   \n",
       "2          Kako UTF-8 rešava problem prenosa podataka?   \n",
       "3    Kako funkcioniše 7-bitna transformaciona šema ...   \n",
       "4          Zašto je UCS-4 plan za budućnost Unicode-a?   \n",
       "..                                                 ...   \n",
       "659              koji su uslovi za takmičarsku košarku   \n",
       "660      razlika u liniji za 3 poena NBA i međunarodna   \n",
       "661                   koliko igrača sme biti na terenu   \n",
       "662            koliko traju poluvremena na fakultetima   \n",
       "663  koliko produžetaka može biti u košarkaškoj uta...   \n",
       "\n",
       "                                                scores  \n",
       "0    {'long_query': 5, 'medium_query': 4, 'short_qu...  \n",
       "1    {'long_query': 5, 'medium_query': 4, 'short_qu...  \n",
       "2    {'long_query': 5, 'medium_query': 5, 'short_qu...  \n",
       "3    {'long_query': 5, 'medium_query': 4, 'short_qu...  \n",
       "4    {'long_query': 4, 'medium_query': 4, 'short_qu...  \n",
       "..                                                 ...  \n",
       "659  {'long_query': 5, 'medium_query': 5, 'short_qu...  \n",
       "660  {'long_query': 5, 'medium_query': 4, 'short_qu...  \n",
       "661  {'long_query': 5, 'medium_query': 5, 'short_qu...  \n",
       "662  {'long_query': 4, 'medium_query': 5, 'short_qu...  \n",
       "663  {'long_query': 4, 'medium_query': 5, 'short_qu...  \n",
       "\n",
       "[664 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['dataset'] = 'wiki'\n",
    "# df_wiki_fixed['dataset'] = 'wiki'\n",
    "# df_science['dataset'] = 'science'\n",
    "df_news['dataset'] = 'news'\n",
    "df_literature['dataset'] = 'literature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pandas.concat([df_wiki, df_news, df_literature], ignore_index=True)\n",
    "# Optionally, you can save the merged dataframe to a file\n",
    "merged_df.to_parquet('datasets/TRAIN14o.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "wiki          664\n",
      "news          636\n",
      "literature    473\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_counts = merged_df['dataset'].value_counts()\n",
    "print(dataset_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>short_query</th>\n",
       "      <th>medium_query</th>\n",
       "      <th>long_query</th>\n",
       "      <th>scores</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UTF-8 UTF-8 varijanta je najzgodnija za kodira...</td>\n",
       "      <td>UTF-8 kodiranje</td>\n",
       "      <td>Unicode fontovi za prenosivost teksta</td>\n",
       "      <td>Kako koristiti UTF-8 u Microsoft Word-u?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Osnovna kodna strana na personalnim računarima...</td>\n",
       "      <td>Kodna strana PC437</td>\n",
       "      <td>Podrška za jezike kodne strane</td>\n",
       "      <td>Kako Unicode rešava problem kodnih strana?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Njen naziv je UCS-2 zato što koristi dva oktet...</td>\n",
       "      <td>UCS-2 karakteri</td>\n",
       "      <td>UTF-16 i problem prostora</td>\n",
       "      <td>Kako UTF-8 rešava problem prenosa podataka?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ova transformaciona šema je prevashodno zgodna...</td>\n",
       "      <td>transformaciona šema latinica</td>\n",
       "      <td>MIME standardi za prenos fajlova</td>\n",
       "      <td>Kako funkcioniše 7-bitna transformaciona šema ...</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Osnovna razlika je u tome što UTF-7 koristi sa...</td>\n",
       "      <td>UTF-7 Base64 prednosti</td>\n",
       "      <td>UCS-4 Unicode standard karakteristike</td>\n",
       "      <td>Zašto je UCS-4 plan za budućnost Unicode-a?</td>\n",
       "      <td>{'long_query': 4, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>wiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>To je opet meni na put stajalo, jer baš to sa...</td>\n",
       "      <td>sažalenje prama Branku</td>\n",
       "      <td>Laura spominje stare stvari</td>\n",
       "      <td>zašto je Fanika bila važna u priči?</td>\n",
       "      <td>{'long_query': 3, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Već gotovo sam se i na Branka počeo jediti, z...</td>\n",
       "      <td>Branko i Laura</td>\n",
       "      <td>Zašto Fani mrzi Branka?</td>\n",
       "      <td>Kako se Branko zaljubio u Lauru umesto Ide?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Pa sad Branko razvede takovu prediku, kakvu jo...</td>\n",
       "      <td>Branko i grofica</td>\n",
       "      <td>Branko i grofica razgovor o Idi</td>\n",
       "      <td>Kako je Branko reagovao na groficine suze?</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>Grofica je silno bogata; pet hiljada da nam d...</td>\n",
       "      <td>grofica Branku novac</td>\n",
       "      <td>zašto Branko ne želi tražiti novac</td>\n",
       "      <td>kako je Branko reagovao na predlog da traži novac</td>\n",
       "      <td>{'long_query': 5, 'medium_query': 5, 'short_qu...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>Zdravković, kao naš husar, sa slugama grofiči...</td>\n",
       "      <td>Zdravković husar</td>\n",
       "      <td>Branko i grobnica Ide</td>\n",
       "      <td>Zdravković sa slugama grofičinim ručao i večerao</td>\n",
       "      <td>{'long_query': 4, 'medium_query': 4, 'short_qu...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1773 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "0     UTF-8 UTF-8 varijanta je najzgodnija za kodira...   \n",
       "1     Osnovna kodna strana na personalnim računarima...   \n",
       "2     Njen naziv je UCS-2 zato što koristi dva oktet...   \n",
       "3     Ova transformaciona šema je prevashodno zgodna...   \n",
       "4     Osnovna razlika je u tome što UTF-7 koristi sa...   \n",
       "...                                                 ...   \n",
       "1768   To je opet meni na put stajalo, jer baš to sa...   \n",
       "1769   Već gotovo sam se i na Branka počeo jediti, z...   \n",
       "1770  Pa sad Branko razvede takovu prediku, kakvu jo...   \n",
       "1771   Grofica je silno bogata; pet hiljada da nam d...   \n",
       "1772   Zdravković, kao naš husar, sa slugama grofiči...   \n",
       "\n",
       "                        short_query                           medium_query  \\\n",
       "0                   UTF-8 kodiranje  Unicode fontovi za prenosivost teksta   \n",
       "1                Kodna strana PC437         Podrška za jezike kodne strane   \n",
       "2                   UCS-2 karakteri              UTF-16 i problem prostora   \n",
       "3     transformaciona šema latinica       MIME standardi za prenos fajlova   \n",
       "4            UTF-7 Base64 prednosti  UCS-4 Unicode standard karakteristike   \n",
       "...                             ...                                    ...   \n",
       "1768         sažalenje prama Branku            Laura spominje stare stvari   \n",
       "1769                 Branko i Laura                Zašto Fani mrzi Branka?   \n",
       "1770               Branko i grofica        Branko i grofica razgovor o Idi   \n",
       "1771           grofica Branku novac     zašto Branko ne želi tražiti novac   \n",
       "1772               Zdravković husar                  Branko i grobnica Ide   \n",
       "\n",
       "                                             long_query  \\\n",
       "0              Kako koristiti UTF-8 u Microsoft Word-u?   \n",
       "1            Kako Unicode rešava problem kodnih strana?   \n",
       "2           Kako UTF-8 rešava problem prenosa podataka?   \n",
       "3     Kako funkcioniše 7-bitna transformaciona šema ...   \n",
       "4           Zašto je UCS-4 plan za budućnost Unicode-a?   \n",
       "...                                                 ...   \n",
       "1768                zašto je Fanika bila važna u priči?   \n",
       "1769        Kako se Branko zaljubio u Lauru umesto Ide?   \n",
       "1770         Kako je Branko reagovao na groficine suze?   \n",
       "1771  kako je Branko reagovao na predlog da traži novac   \n",
       "1772   Zdravković sa slugama grofičinim ručao i večerao   \n",
       "\n",
       "                                                 scores     dataset  \n",
       "0     {'long_query': 5, 'medium_query': 4, 'short_qu...        wiki  \n",
       "1     {'long_query': 5, 'medium_query': 4, 'short_qu...        wiki  \n",
       "2     {'long_query': 5, 'medium_query': 5, 'short_qu...        wiki  \n",
       "3     {'long_query': 5, 'medium_query': 4, 'short_qu...        wiki  \n",
       "4     {'long_query': 4, 'medium_query': 4, 'short_qu...        wiki  \n",
       "...                                                 ...         ...  \n",
       "1768  {'long_query': 3, 'medium_query': 4, 'short_qu...  literature  \n",
       "1769  {'long_query': 5, 'medium_query': 5, 'short_qu...  literature  \n",
       "1770  {'long_query': 5, 'medium_query': 5, 'short_qu...  literature  \n",
       "1771  {'long_query': 5, 'medium_query': 5, 'short_qu...  literature  \n",
       "1772  {'long_query': 4, 'medium_query': 4, 'short_qu...  literature  \n",
       "\n",
       "[1773 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ev, dataset_counts_train, dataset_counts_eval = get_train_and_eval_datasets(dataset_name=\"datasets/TRAIN11k_fixed_v2.parquet\", question_type=\"short_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "wiki          0.632590\n",
      "science       0.180130\n",
      "news          0.150464\n",
      "literature    0.036816\n",
      "Name: count, dtype: float64\n",
      "dataset\n",
      "wiki          0.647034\n",
      "science       0.173709\n",
      "news          0.145113\n",
      "literature    0.034144\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dataset_proportions = dataset_counts_train / dataset_counts_train.sum()\n",
    "\n",
    "# Print the proportions\n",
    "print(dataset_proportions)\n",
    "\n",
    "dataset_proportions = dataset_counts_eval / dataset_counts_eval.sum()\n",
    "\n",
    "# Print the proportions\n",
    "print(dataset_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive'],\n",
       "    num_rows: 9371\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Šta je uticalo na loše rezultate poslovanja?'\n",
      "('319 stabilno poslovanje Drugi stratum čini takođe šest velikih privrednih '\n",
      " 'subjekata koji su u istom vremenskom okviru imali narušenu finansijsku '\n",
      " 'strukturu, što se direktno odrazilo i na loše rezutate poslovanja Na bazi '\n",
      " 'predloženih kompanija formirane su dve polarizovane grupe velikih privrednih '\n",
      " 'subjekata kako bi putem predloženog modela mogla da se pokaže homogenost u '\n",
      " 'kretanju kvantitativnih i kvalitativnih pokazatelja Takođe, polarizovani '\n",
      " 'pristup u istraživanju dao je mogućnost dodatnog testiranja pouzdanosti '\n",
      " 'predloženog modela ocene kreditnog boniteta velikih privrednih subjekata')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(tr['anchor'][2])\n",
    "pprint(tr['positive'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'score'],\n",
       "    num_rows: 1128\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jerteh/Jerteh-355 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length for the model is: 514\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load the model\n",
    "model_name = \"jerteh/Jerteh-355\"  # Replace this with the correct model identifier\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model = model.half()\n",
    "# Access the max sequence length\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "\n",
    "print(f\"The maximum sequence length for the model is: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_table = pq.read_table('datasets/TRAIN11k_fixed_v2.parquet')\n",
    "train = loaded_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonizacija definicija grada - ESPON 2013 Koncepti evropskih statističkih jedinica analize (NUTS; LAU), urbanih morfoloških područja funkcionalnih urbanih područja (FUA) prihvaćeni su, sinhronizovani i dalje razvijani u sklopu projekata ESPON 2013 Database (ESPON, 2013d; ESPON Database 2013, 2014; ESPON, 2013c) Evropske posmatračke mreže za teritorijalni razvoj i koheziju (ESPON - European Observation Network for Territorial Development and Cohesion) Baza podataka koja je nastala kao rezultat ovog projekta dalje se razvila kroz ESPON M4D (Multi Dimension Database Design and Development) projekat, koji za glavni cilj ima održavanje, ažuriranje, razvoj i proširenje baze podataka ESPON 2013 (ESPON, ESPON baza pruža temeljne regionalne informacije obezbjeđene iz ESPON-ovih projekata i od strane Eurostata, a koje se mogu koristiti kao podrška za teritorijalnu analizu razvoja na različitim geografskim nivoima\n",
      "Od 1874. do 1882. godine, pohađao je gimnaziju. Za to vreme, napisao je svoje prve kompozicije, prva kamerna dela. Njegovo školovanje, međutim, bilo je pod strogim očevim nadzorom i uglavnom ograničeno na dela klasičnih kompozitora: Hajdna, Mocarta i Betovena. Tek u šesnaestoj godini je Rihard uspeo da se, uprkos očevom negodovanju, dočepa partiture Vagnerovog \"Tristana\", koji mu je istinski otvorio uši za snagu muzike. Zanimljivo je, dakle, da je Štraus tako brzo uspeo da se otrese od oca nasleđene mržnje prema Vagneru (ranije je za \"Zigfrida\" rekao: \"Tu nema ni traga koherentne melodije... potpuni haos, rekao bih\")..\n",
      " Iz Nikšića je i Filip Vučić, jedini predstavnik državne zajednice Srbija i Crna Gora na evroviziji za djecu Nikšić, kao izuzetno inspirativna sredina, nosi epitet grada velikih umjetnika i zvučnih imena kulturnih poslenika U oblasti slikarstva nezaobilazno ime je Vojo Stanić, čuveni jugoslovenski i crnogorski slikar, koji je u Nikšiću živio do završetka gimnazije i koji je rekao da mu je život u ovom gradu odredio slikarski stil i koji je uvijek isticao da sebe prije svega smatra Nikšićaninom\n",
      " Dobijen je niz polusintetskih penicilina, ali značajniju ulogu u kliničkom okruženju ima oko desetak predstavnika Dole je dat nepotpun spisak polusintetskih penicilina, grupisanih po izvesnim zajedničkim osobinama Ovo su penicilini proširenog spektra, prigodni za \"-{per os}-\" primenu, ali osetljivi na dejstvo penicilaza Rastvorne soli mogu se primenjivati i parenteralno, kod težih infekcija Sa pojavom sojeva koji luče beta-laktamaze, terapijska vrednost pojedinih penicilina znatno je smanjena jer više nisu pokazivali dobre rezultate protiv intrinsički podložnih bakterija\n",
      " cit, str 130 U slučaju da izvršni poverilac ne uspe da dokaže javnom ili po zakonu overenom ispravom da je ispunio svoju obavezu ili da je uslov nastupio, bio bi u obavezi da pokrene parnični postupak radi utvrđenja da je na osnovu izvršne isprave ovlašćen tražiti i zahtevati bezuslovno izvršenje radi ostvarenja svog potraživanja Radi se o pravu utvrđenja svojevrsnog izvršnog pravozaštitnog zahteva kao javnopravnog ovlašćenja na izvršnopravnu zaštitu – svojevrsna purifikacijska tužba\n"
     ]
    }
   ],
   "source": [
    "for context in train.sample(5)['context']:\n",
    "    print(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
