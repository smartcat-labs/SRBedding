{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer,  models, util\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/selena/Datasets/SRBendding')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Path(\"~/Datasets/SRBendding\").expanduser()\n",
    "dataset.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_dataset(\"stsb_multi_mt\", name=\"fr\", split=\"train\", cache_dir=dataset)\n",
    "df_valid = load_dataset(\"stsb_multi_mt\", name=\"fr\", split=\"dev\", cache_dir=dataset)\n",
    "df_test = load_dataset(\"stsb_multi_mt\", name=\"fr\", split=\"test\", cache_dir=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in df_train:\n",
    "#     print(df)  # {'sentence1': 'Un avion est en train de décoller.', 'sentence2': 'Un avion est en train de décoller.', 'similarity_score': 5.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    dataset_samples=[]\n",
    "    for df in dataset:\n",
    "        score = float(df['similarity_score'])/5.0  # Normalize score to range 0 ... 1\n",
    "        # sta bi ovo bilo kod nas.. mi nemamo score ili da pravimo score, da napravimo ukrstanja\n",
    "        inp_example = InputExample(texts=[df['sentence1'], \n",
    "                                    df['sentence2']], label=score)\n",
    "        dataset_samples.append(inp_example)\n",
    "    return dataset_samples\n",
    "\n",
    "train_samples = convert_dataset(df_train)\n",
    "dev_samples = convert_dataset(df_valid)\n",
    "test_samples = convert_dataset(df_test)\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos_score_transformation: This is a transformation function applied to the cosine similarity score before calculating the loss. By default, it is nn.Identity(), which means no transformation is applied.  \n",
    "\n",
    "sentence_features: This is an iterable containing two dictionaries, each representing the features of a sentence (typically a tokenized version of the sentence). The model processes these features to generate sentence embeddings.  \n",
    "\n",
    "labels: This is a tensor containing the target similarity scores (labels) for each pair of sentences.  \n",
    "\n",
    "\n",
    "za poslednju liniju koda  \n",
    "The transformed cosine similarity (output) is compared to the target labels using the loss function (self.loss_fct). The .view(-1) ensures that the label tensor is flattened if necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    CosineSimilarityLoss expects, that the InputExamples consists of two texts and a float label.\n",
    "    It computes the vectors u = model(input_text[0]) and v = model(input_text[1]) and measures the cosine-similarity between the two.\n",
    "    Minimizes the following loss: \n",
    "                   ||input_label - cos_score_transformation(cosine_sim(u,v))||_2.\n",
    "    :param model: SentenceTranformer model\n",
    "    :param loss_fct: loss function is used to compare the cosine_similartiy(u,v) with the input_label. \n",
    "                  MSE = ||input_label - cosine_sim(u,v)||_2\n",
    "    :param cos_score_transformation: The cos_score_transformation function is applied on top of cosine_similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, model: SentenceTransformer, \n",
    "                 loss_fct = nn.MSELoss(), # je l treba loss neki drugi\n",
    "                 cos_score_transformation=nn.Identity()):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_fct = loss_fct\n",
    "        self.cos_score_transformation = cos_score_transformation\n",
    "\n",
    "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
    "        embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
    "        output = self.cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))\n",
    "        return self.loss_fct(output, labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =  \"camembert/camembert-large\"\n",
    "model_save_path = 'output/training_stsbenchmark_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "max_seq_length = 128\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "\"\"\"Performs pooling (max or mean) on the token embeddings.\n",
    "  Iit generates from a variable sized sentence a fixed sized sentence embedding, \n",
    "  allows to use the CLS token if it is returned by the underlying word embedding model.\n",
    "  We can concatenate multiple poolings together.\n",
    "  - word_embedding_dimension: Dimensions for the word embeddings\n",
    "  - pooling_mode_cls_token: Use the first token (CLS token) as text representations\n",
    "  - pooling_mode_max_tokens: Use max in each dimension over all tokens.\n",
    "  - pooling_mode_mean_tokens: Perform mean-pooling\n",
    "  \"\"\"\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False,\n",
    "                               pooling_mode_mean_tokens=True)\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
